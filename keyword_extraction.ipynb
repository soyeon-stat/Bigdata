{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거, 형태소 분석기로 명사 추출, title/content/comments 가중치 부여여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드 추출 및 저장이 완료.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "import requests\n",
    "\n",
    "# 불용어 리스트 가져오기\n",
    "stopwords_url = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-ko/master/stopwords-ko.json\"\n",
    "stopwords = set(requests.get(stopwords_url).json())\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 가중치 설정\n",
    "weights = {\n",
    "    'title': 2.0,\n",
    "    'content': 1.5,\n",
    "    'comments': 1.0\n",
    "}\n",
    "\n",
    "# 키워드 추출 함수\n",
    "def extract_keywords(text, weight):\n",
    "    nouns = okt.nouns(text)\n",
    "    return Counter({noun: count * weight for noun, count in Counter(nouns).items()})\n",
    "\n",
    "# JSON 데이터 불러오기\n",
    "with open(\"theqoo_crawling_1102_1108.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 각 게시글에 대해 키워드 추출 및 추가\n",
    "for post in data:\n",
    "    total_counter = Counter()\n",
    "    \n",
    "    # 제목에서 키워드 추출\n",
    "    if 'title' in post:\n",
    "        total_counter.update(extract_keywords(post['title'], weights['title']))\n",
    "    \n",
    "    # 본문에서 키워드 추출\n",
    "    if 'content' in post:\n",
    "        total_counter.update(extract_keywords(post['content'], weights['content']))\n",
    "    \n",
    "    # 댓글에서 키워드 추출\n",
    "    if 'comments' in post and isinstance(post['comments'], list):\n",
    "        for comment in post['comments']:\n",
    "            total_counter.update(extract_keywords(comment, weights['comments']))\n",
    "    \n",
    "    # 불용어 제거 및 최소 등장 횟수 필터링\n",
    "    keywords = [word for word, count in total_counter.items() if word not in stopwords and count >= 5]\n",
    "    \n",
    "    # 키워드 리스트를 게시글에 추가\n",
    "    post['keywords'] = keywords\n",
    "\n",
    "# 결과를 새로운 JSON 파일로 저장\n",
    "with open(\"data_with_keywords_v1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"키워드 추출 및 저장이 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konply.tag.Okt로 명사 추출\n",
    "# KMeans 군집화 사용하여 벡터화된 단어 군집화, 각 군집에서 중요 단어 추출\n",
    "# TfidfVectorizer를 사용하여 군집 내 중요 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드 추출 및 저장 완료\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# KoBERT 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "model = BertModel.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# KoBERT 임베딩 생성 함수\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "# 명사 추출 함수\n",
    "def extract_nouns(text):\n",
    "    return okt.nouns(text)\n",
    "\n",
    "# 데이터 로드\n",
    "with open(\"theqoo_crawling_1102_1108.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 게시글 10개만 테스트\n",
    "data = data[:10]\n",
    "\n",
    "# 게시글 데이터를 처리\n",
    "for post in data:\n",
    "    title = post.get(\"title\", \"\")\n",
    "    content = post.get(\"content\", \"\")\n",
    "    comments = post.get(\"comments\", [])\n",
    "    \n",
    "    # 텍스트에서 명사 추출\n",
    "    nouns = []\n",
    "    if title:\n",
    "        nouns.extend(extract_nouns(title))\n",
    "    if content:\n",
    "        nouns.extend(extract_nouns(content))\n",
    "    for comment in comments:\n",
    "        nouns.extend(extract_nouns(comment))\n",
    "    \n",
    "    # 명사를 벡터화 (KoBERT)\n",
    "    noun_embeddings = []\n",
    "    for noun in nouns:\n",
    "        try:\n",
    "            embedding = get_embedding(noun)\n",
    "            noun_embeddings.append((noun, embedding))\n",
    "        except Exception:\n",
    "            continue  # KoBERT에서 처리할 수 없는 단어는 무시\n",
    "\n",
    "    if not noun_embeddings:\n",
    "        post[\"keywords\"] = []\n",
    "        continue\n",
    "\n",
    "    # KoBERT 임베딩 추출\n",
    "    words, embeddings = zip(*noun_embeddings)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "\n",
    "    # KMeans로 군집화\n",
    "    n_clusters = min(len(embeddings), 3)  # 클러스터 수는 최대 3으로 제한\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    # 군집별 단어 빈도 계산\n",
    "    cluster_nouns = {i: [] for i in range(n_clusters)}\n",
    "    for word, cluster in zip(words, cluster_labels):\n",
    "        cluster_nouns[cluster].append(word)\n",
    "\n",
    "    # 각 클러스터에서 TF-IDF로 상위 10개 키워드 추출\n",
    "    final_keywords = []\n",
    "    for cluster, cluster_words in cluster_nouns.items():\n",
    "        if cluster_words:  # 군집 내 단어가 존재할 경우에만 처리\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            try:\n",
    "                tfidf_matrix = vectorizer.fit_transform(cluster_words)\n",
    "                sorted_indices = np.argsort(tfidf_matrix.toarray().sum(axis=0))[::-1]\n",
    "                top_keywords = [vectorizer.get_feature_names_out()[i] for i in sorted_indices[:10]]\n",
    "                final_keywords.extend(top_keywords)\n",
    "            except ValueError:\n",
    "                continue  # 군집 단어가 모두 불용어일 경우 무시\n",
    "\n",
    "    # 최종 상위 10개의 키워드 선택\n",
    "    post[\"keywords\"] = list(set(final_keywords))[:10]\n",
    "\n",
    "# 결과 저장\n",
    "with open(\"data_with_keywords_v2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"키워드 추출 및 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 대비 title, content, comments 가중치 추가만 다름\n",
    "# konply.tag.Okt로 명사 추출\n",
    "# KMeans 군집화 사용하여 벡터화된 단어 군집화, 각 군집에서 중요 단어 추출\n",
    "# TfidfVectorizer를 사용하여 군집 내 중요 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드 추출 및 저장 완료\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# KoBERT 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "model = BertModel.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# KoBERT 임베딩 생성 함수\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "# 명사 추출 함수\n",
    "def extract_nouns(text):\n",
    "    return okt.nouns(text)\n",
    "\n",
    "# 입력 데이터 로드\n",
    "input_file = \"theqoo_crawling_1102_1108.json\"\n",
    "output_file = \"data_with_keywords_v3.json\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 게시글 10개만 테스트\n",
    "data = data[:10]\n",
    "\n",
    "# 가중치 설정\n",
    "weights = {\n",
    "    'title': 2.0,\n",
    "    'content': 1.5,\n",
    "    'comments': 1.0\n",
    "}\n",
    "\n",
    "# 게시글 데이터를 처리\n",
    "for post in data:\n",
    "    title = post.get(\"title\", \"\")\n",
    "    content = post.get(\"content\", \"\")\n",
    "    comments = post.get(\"comments\", [])\n",
    "    \n",
    "    # 텍스트에서 명사 추출 및 가중치 적용\n",
    "    weighted_nouns = []\n",
    "    if title:\n",
    "        for noun in extract_nouns(title):\n",
    "            weighted_nouns.extend([noun] * int(weights['title']))  # title에 가중치 적용\n",
    "    if content:\n",
    "        for noun in extract_nouns(content):\n",
    "            weighted_nouns.extend([noun] * int(weights['content']))  # content에 가중치 적용\n",
    "    for comment in comments:\n",
    "        for noun in extract_nouns(comment):\n",
    "            weighted_nouns.append(noun)  # comments는 기본 가중치\n",
    "    \n",
    "    # 명사를 벡터화 (KoBERT)\n",
    "    noun_embeddings = []\n",
    "    for noun in weighted_nouns:\n",
    "        try:\n",
    "            embedding = get_embedding(noun)\n",
    "            noun_embeddings.append((noun, embedding))\n",
    "        except Exception:\n",
    "            continue  # KoBERT에서 처리할 수 없는 단어는 무시\n",
    "\n",
    "    if not noun_embeddings:\n",
    "        post[\"keywords\"] = []\n",
    "        continue\n",
    "\n",
    "    # KoBERT 임베딩 추출\n",
    "    words, embeddings = zip(*noun_embeddings)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "\n",
    "    # KMeans로 군집화\n",
    "    n_clusters = min(len(embeddings), 3)  # 클러스터 수는 최대 3으로 제한\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    # 군집별 단어 빈도 계산\n",
    "    cluster_nouns = {i: [] for i in range(n_clusters)}\n",
    "    for word, cluster in zip(words, cluster_labels):\n",
    "        cluster_nouns[cluster].append(word)\n",
    "\n",
    "    # 각 군집에서 TF-IDF로 상위 10개 키워드 추출\n",
    "    final_keywords = []\n",
    "    for cluster, cluster_words in cluster_nouns.items():\n",
    "        if cluster_words:  # 군집 내 단어가 존재할 경우에만 처리\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            try:\n",
    "                tfidf_matrix = vectorizer.fit_transform(cluster_words)\n",
    "                sorted_indices = np.argsort(tfidf_matrix.toarray().sum(axis=0))[::-1]\n",
    "                top_keywords = [vectorizer.get_feature_names_out()[i] for i in sorted_indices[:10]]\n",
    "                final_keywords.extend(top_keywords)\n",
    "            except ValueError:\n",
    "                continue  # 군집 단어가 모두 불용어일 경우 무시\n",
    "\n",
    "    # 최종 상위 10개의 키워드 선택\n",
    "    post[\"keywords\"] = list(set(final_keywords))[:10]\n",
    "\n",
    "# 결과 저장\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"키워드 추출 및 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT 개선 적용\n",
    "# 불용어 제거기능 빠져있어서 추가\n",
    "# 배치 처리와 메모리 최적화: KoBERT 임베딩을 한 번에 처리하도록 최적화하여 속도와 메모리 사용을 개선합니다.\n",
    "# 에러 로깅: 예외 처리 시 로깅을 추가하여 문제를 추적할 수 있도록 합니다.\n",
    "# TF-IDF 개선: 군집화 전 전체 데이터에 대해 한 번만 TF-IDF 벡터화를 수행하여 성능을 개선합니다.\n",
    "# 가독성 향상: 중복 코드를 함수로 정리하여 코드의 가독성을 높입니다.\n",
    "# 동적 가중치 및 클러스터 수 조정: 동적으로 가중치를 설정하거나 클러스터 수를 조정하여 더 다양한 데이터에 유연하게 대응합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드 추출 및 저장 완료\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "# 불용어 리스트를 URL에서 가져오기\n",
    "stopwords_url = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-ko/master/stopwords-ko.json\"\n",
    "\n",
    "# 불용어 목록 로드\n",
    "response = requests.get(stopwords_url)\n",
    "STOPWORDS = set(response.json())\n",
    "\n",
    "# KoBERT 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "model = BertModel.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# KoBERT 임베딩 생성 함수\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "# 명사 추출 함수\n",
    "def extract_nouns(text):\n",
    "    return okt.nouns(text)\n",
    "\n",
    "# 입력 데이터 로드\n",
    "input_file = \"theqoo_crawling_1102_1108.json\"\n",
    "output_file = \"data_with_keywords_v4.json\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 게시글 10개만 테스트\n",
    "data = data[:10]\n",
    "\n",
    "# 가중치 설정\n",
    "weights = {\n",
    "    'title': 2.0,\n",
    "    'content': 1.5,\n",
    "    'comments': 1.0\n",
    "}\n",
    "\n",
    "# 게시글 데이터를 처리\n",
    "for post in data:\n",
    "    title = post.get(\"title\", \"\")\n",
    "    content = post.get(\"content\", \"\")\n",
    "    comments = post.get(\"comments\", [])\n",
    "    \n",
    "    # 텍스트에서 명사 추출 및 가중치 적용\n",
    "    weighted_nouns = []\n",
    "    if title:\n",
    "        for noun in extract_nouns(title):\n",
    "            weighted_nouns.extend([noun] * int(weights['title']))  # title에 가중치 적용\n",
    "    if content:\n",
    "        for noun in extract_nouns(content):\n",
    "            weighted_nouns.extend([noun] * int(weights['content']))  # content에 가중치 적용\n",
    "    for comment in comments:\n",
    "        for noun in extract_nouns(comment):\n",
    "            weighted_nouns.append(noun)  # comments는 기본 가중치\n",
    "    \n",
    "    # 명사를 벡터화 (KoBERT)\n",
    "    noun_embeddings = []\n",
    "    for noun in weighted_nouns:\n",
    "        try:\n",
    "            embedding = get_embedding(noun)\n",
    "            noun_embeddings.append((noun, embedding))\n",
    "        except Exception:\n",
    "            continue  # KoBERT에서 처리할 수 없는 단어는 무시\n",
    "\n",
    "    if not noun_embeddings:\n",
    "        post[\"keywords\"] = []\n",
    "        continue\n",
    "\n",
    "    # KoBERT 임베딩 추출\n",
    "    words, embeddings = zip(*noun_embeddings)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "\n",
    "    # KMeans로 군집화\n",
    "    n_clusters = min(len(embeddings), 3)  # 클러스터 수는 최대 3으로 제한\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    # 군집별 단어 빈도 계산\n",
    "    cluster_nouns = {i: [] for i in range(n_clusters)}\n",
    "    for word, cluster in zip(words, cluster_labels):\n",
    "        cluster_nouns[cluster].append(word)\n",
    "\n",
    "    # 각 군집에서 TF-IDF로 상위 10개 키워드 추출\n",
    "    final_keywords = []\n",
    "    for cluster, cluster_words in cluster_nouns.items():\n",
    "        if cluster_words:  # 군집 내 단어가 존재할 경우에만 처리\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            try:\n",
    "                # cluster_words를 하나의 문자열로 병합\n",
    "                combined_text = \" \".join(cluster_words)\n",
    "                tfidf_matrix = vectorizer.fit_transform([combined_text])\n",
    "                sorted_indices = np.argsort(tfidf_matrix.toarray().sum(axis=0))[::-1]\n",
    "                top_keywords = [vectorizer.get_feature_names_out()[i] for i in sorted_indices[:10]]\n",
    "                final_keywords.extend(top_keywords)\n",
    "            except ValueError:\n",
    "                continue  # 군집 단어가 모두 불용어일 경우 무시\n",
    "\n",
    "    # 최종 상위 10개의 키워드 선택\n",
    "    post[\"keywords\"] = list(set(final_keywords))[:10]\n",
    "\n",
    "# 결과 저장\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"키워드 추출 및 저장 완료\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
