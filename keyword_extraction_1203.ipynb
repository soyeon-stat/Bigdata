{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# JSON 파일 병합 및 필터링\n",
    "folder_path = \"./\"  # 동일 폴더\n",
    "start_date = \"2024-11-02\"\n",
    "end_date = \"2024-11-12\"\n",
    "\n",
    "# JSON 병합\n",
    "all_data = []\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(folder_path, file_name), \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "            for post in data:\n",
    "                date_key = next((key for key in [\"post_date\", \"작성일\", \"date\"] if key in post), None)\n",
    "                if date_key:\n",
    "                    post_date = post[date_key]\n",
    "                    all_data.append({\n",
    "                        \"date\": post_date,\n",
    "                        \"content\": post.get(\"content\", \"\"),\n",
    "                        \"comments\": post.get(\"comments\", \"\"),\n",
    "                        \"title\": post.get(\"title\", \"\"),\n",
    "                        \"community\": post.get(\"community\", file_name.split(\".json\")[0])\n",
    "                    })\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(all_data)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"date\"])  # 날짜 파싱 실패 행 제거\n",
    "df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트 로드\n",
    "stopwords_url = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-ko/master/stopwords-ko.json\"\n",
    "response = requests.get(stopwords_url)\n",
    "STOPWORDS = set(response.json())\n",
    "STOPWORDS.update(['지금', '우리', '그때', '처음', '보고', '삭제', '진짜', '다시'])\n",
    "\n",
    "# 형태소 분석기 초기화 (Okt)\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과가 word_analysis_v2.json에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 명사 및 고유명사 추출 함수 (최적화)\n",
    "def extract_nouns_optimized(text, stopwords):\n",
    "    if not text:  # 빈 텍스트 처리\n",
    "        return []\n",
    "    pos_tags = okt.pos(text, norm=True, stem=True)  # 표준화 및 어간 추출\n",
    "    return [\n",
    "        word for word, tag in pos_tags\n",
    "        if tag in ['Noun', 'ProperNoun'] and word not in stopwords and len(word) > 1\n",
    "    ]  # 필터링 단계 간소화\n",
    "\n",
    "# 데이터프레임의 모든 텍스트 병합 및 명사 추출\n",
    "def process_dataframe(df, stopwords):\n",
    "    word_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        combined_text = f\"{row['content']} {row['comments']} {row['title']}\"\n",
    "        nouns = extract_nouns_optimized(combined_text, stopwords)\n",
    "        word_data.extend([(noun, row[\"date\"].strftime(\"%Y-%m-%d\"), row[\"community\"]) for noun in nouns])\n",
    "    return word_data\n",
    "\n",
    "# 단어별 집계 및 결과 정리\n",
    "def generate_results(word_data):\n",
    "    word_counter = Counter([word[0] for word in word_data])\n",
    "    result = [\n",
    "        {\n",
    "            \"word\": word,\n",
    "            \"count\": count,\n",
    "            \"dates\": list(set(d[1] for d in word_data if d[0] == word)),\n",
    "            \"communities\": list(set(d[2] for d in word_data if d[0] == word))\n",
    "        }\n",
    "        for word, count in word_counter.most_common()\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "# 불용어 리스트와 데이터 처리\n",
    "word_data = process_dataframe(df, STOPWORDS)\n",
    "result = generate_results(word_data)\n",
    "\n",
    "# 결과 저장\n",
    "output_file = \"word_analysis_v2.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(result, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"결과가 {output_file}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출된 단어 목록:\n",
      "['사람', '댓글', '새끼', '일시', '그냥', '생각', '한국', '존나', '여자', '병신', '미국', '정도', '남자', '소리', '문제', '나라', '트럼프', '지랄', '저런', '중국', '때문', '이제', '일본', '수준', '시발', '씨발', '제발', '요즘', '자체', '회사']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# word_analysis.json 파일에서 단어만 출력\n",
    "output_file = \"word_analysis_v2.json\"\n",
    "\n",
    "with open(output_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 단어만 추출하여 출력\n",
    "words = [entry[\"word\"] for entry in data]\n",
    "words_30th = words[:30]\n",
    "print(\"추출된 단어 목록:\")\n",
    "print(words_30th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "import pandas as pd\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "data = pd.read_json('word_analysis_v2.json')\n",
    "\n",
    "data = data.loc[data['count'] >= 100, :]\n",
    "data['pos'] = [kkma.pos(w) for w in data['word']]\n",
    "data['NNP'] = data['pos'].apply(lambda x : \"NNP\" in str(x))\n",
    "data.to_excel('고유명사.xlsx', index = False)\n",
    "data.loc[data['NNP'] == True, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
